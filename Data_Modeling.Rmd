---
title: "Modeling Approaches"
author: "Natalia Lara"
date: "4/1/2023"
output:
  html_document:
    toc: true
    code_folding: hide
    number_sections: true
    toc_depth: 2
    toc_float:
      collapsed: true
      smooth_scroll: true
bibliography: biblio.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyr)
library(stringr)
library(brms)
library(readr)
library(plotROC)
library(kableExtra)
library(ggdendro)
library(caret)
library(plotly)
library(PerformanceAnalytics)
library(pROC)
library(ISLR2)
library(leaps)
library(bayestestR)
library(bestglm)
library(glmulti)

get_roc_curve <- function(mod){
  
  test_pred_20 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_20 <- as_factor(if_else(test_pred_20 < 0.2, 'SI', 'NO'))
  
  
  test_pred_30 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_30 <- as_factor(if_else(test_pred_30 < 0.3, 'SI', 'NO'))
  
    test_pred_50 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_50 <- as_factor(if_else(test_pred_50 < 0.5, 'SI', 'NO'))
  
  test_pred_70 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_70 <- as_factor(if_else(test_pred_70 < 0.7, 'SI', 'NO'))
  
  test_pred_90 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_90 <- as_factor(if_else(test_pred_90 < 0.9, 'SI', 'NO'))
  
  # Confusion Matrix:
  
  test_con_mat_20 = confusionMatrix(test_pred_20,
                                    datos$CONTROLADO)

  test_con_mat_30 = confusionMatrix(test_pred_30,
                                    datos$CONTROLADO)
  
  test_con_mat_50 = confusionMatrix(test_pred_50,
                                    datos$CONTROLADO)
  
  test_con_mat_70 = confusionMatrix(test_pred_70,
                                    datos$CONTROLADO)
  
  test_con_mat_90 = confusionMatrix(test_pred_90,
                                    datos$CONTROLADO)
  
  # Metrics:
  
  metrics = rbind(
  
  c(test_con_mat_20$overall["Accuracy"], 
    test_con_mat_20$byClass["Sensitivity"], 
    test_con_mat_20$byClass["Specificity"]),
  
  c(test_con_mat_30$overall["Accuracy"], 
    test_con_mat_30$byClass["Sensitivity"], 
    test_con_mat_30$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  c(test_con_mat_70$overall["Accuracy"], 
    test_con_mat_70$byClass["Sensitivity"], 
    test_con_mat_70$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])
  )
  
  rownames(metrics) = c('c = 0.20', 'c = 0.30', 'c = 0.50',
                        'c = 0.70', 'c = 0.90')
  
  # ROC curve:
  
  test_prob = predict(mod,
                      newdata = datos,
                      type = "response")
  
  test_roc = roc(datos$CONTROLADO ~ test_prob, plot = TRUE, print.auc = TRUE)
  
}
```

# Introduction

This document presents some preliminary analysis of the mobility dataset gathered in 2019 at the Military Hospital. 

based on the spinal impairments using metric tape and goniometers. However, the obtained measurements has systematic errors, low accuracy, and it is user-dependent. 

Optical systems have been developed from new technologies in the field of sensors and motion capture. These systems can gather high precision data and it is not user-dependent. 

# Data 

The study was conducted on 21 patients and 21 controls with different socio-demographic characteristics. Both methods were applied at ECCI's  Biomechanics Lab. The manual method was carried out before the optimcal system. The data and results derived from the manual system were carried out by trained personnel from the Military Hospital. 

The optical system had 6 Optitrack Flex 3 cameras with an acquisition rate of 100 fps. The calibration and acquisition of the kinematic data was carried out through the Motive software and, later, the data were processed through the Visual 3D software, to obtain the spinal mobility results. The experimental protocol included 29 reflective markers placed on the patient or individual's body.

## Data

The following table presents the variables that were measurements for the controls and patients with gait disorder:

```{r data1}

patients <- read_csv('Data/patients.csv')
patients$NUMERO <- as_factor(patients$NUMERO)
patients$ID <- as_factor(patients$ID)
patients$BIOLOGICO <- as_factor(patients$BIOLOGICO)
patients$TIPO <- as_factor(patients$TIPO)
patients$CONTROLADO <- as_factor(patients$CONTROLADO)

#-------------------------------------------------------------------------------

controls <- read_csv('Data/controls.csv')
controls$NUMERO <- as_factor(controls$NUMERO)
controls$ID <- as_factor(controls$ID)
controls$BIOLOGICO <- as_factor(controls$BIOLOGICO)
controls$TIPO <- as_factor(controls$TIPO)
controls$CONTROLADO <- 'SI'

datos <- rbind(patients, controls)

datos <- datos %>%
  select(PESO_kg, ROTA_CERV_LAB_IZQUIERDA, ROTA_CERV_LAB_DERECHA,
         FLEX_FRONT_CERV_LAB_GRAD_FLEXION, FLEX_FRONT_CERV_LAB_GRAD_EXTESION,
         FLEX_LAT_CERV_LAB_GRAD_DERECHA, FLEX_LAT_CERV_LAB_GRAD_IZQUIERDA,
         SHOBERT_MODIFICADO_CM_LABORATORIO, FLEX_LUMBAR_LAT_LAB_CM_DERECHA,
         FLEX_LUMBAR_LAT_LAB_CM_IZQUIERDA, DISTANCIA_INTERMALEOLAR_LAB_CM,
         CONTROLADO)

names(datos)[2] <- 'ROTA_CERV_IZQUIERDA'                     # in degrees
names(datos)[3] <- 'ROTA_CERV_DERECHA'                       # in degrees
names(datos)[4] <- 'FLEX_FRONT_CERV_FLEXION'                 # in degrees
names(datos)[5] <- 'FLEX_FRONT_CERV_EXTENSION'               # in degrees
names(datos)[6] <- 'FLEX_LAT_CERV_DERECHA'                   # in degrees
names(datos)[7] <- 'FLEX_LAT_CERV_IZQUIERDA'                 # in degrees
names(datos)[8] <- 'SCHOBERT_MOD'                            # in cm
names(datos)[9] <- 'FLEX_LUMBAR_LAT_DERECHA'                 # in cm
names(datos)[10] <- 'FLEX_LUMBAR_LAT_IZQUIERDA'              # in cm
names(datos)[11] <- 'DISTANCIA_INTERMALEOLAR'                # in cm

datos$CONTROLADO <- as_factor(datos$CONTROLADO)

datos <- datos[complete.cases(datos),]

datos <- datos %>%
    as_tibble() %>%
    mutate(across(where(is.numeric), scale))

datos %>%
  kbl(booktabs = T) %>%
  kable_material(c('hover', 'striped'), full_width = F) %>%
  scroll_box(width = "100%", height = "350px")

```

A correlation analysis provides an idea about the symmetric (assymmetric) characteristics of the sampling units. Note that there are variables that were measured on both sides, and thus, a healthy and symmetric patients tend to be highly symmetric in these markers:

```{r fig.width=15, fig.height=15}
chart.Correlation(datos[ , -(12)], histogram = T)
```
```{r}
datos %>%
  filter(ROTA_CERV_DER > 25, ROTA_CERV_DER < 60) %>%
  ggplot(aes(x = ROTA_CERV_DER, y = FLEX_LAT_CERV_DERECHA)) +
  geom_point(aes(colour = CONTROLADO)) +
  geom_smooth(method = 'lm')

datos %>%
  ggplot(aes(x = ROTA_CERV_DER, y = FLEX_LAT_CERV_DERECHA)) +
  geom_point(aes(colour = CONTROLADO)) +
  theme(legend.position = 'bottom')

datos %>%
  ggplot(aes(x = ROTA_CERV_DER, y = SCHOBERT_MOD)) +
  geom_point(aes(colour = CONTROLADO)) +
  theme(legend.position = 'bottom')

datos %>%
  ggplot(aes(x = DISTANCIA_INTERMALEOLAR, y = PESO_kg)) +
  geom_point(aes(colour = CONTROLADO)) +
  theme(legend.position = 'bottom')

datos %>%
  filter(ROTA_CERV_DER > 25, ROTA_CERV_DER < 60) %>%
  mutate(correlation = cor(ROTA_CERV_DER, FLEX_LAT_CERV_DERECHA))

cor(datos$ROTA_CERV_DER, datos$FLEX_LAT_CERV_DERECHA)
```


## One-to-One relationships



```{r}

FLEX_LUMBAR_LAT_MANUAL_CM_DERECHA <- c(patients$FLEX_LUMBAR_LAT_MANUAL_CM_DERECHA,
                                       controls$FLEX_LUMBAR_LAT_MANUAL_CM_DERECHA)

FLEX_LUMBAR_LAT_LAB_CM_DERECHA <- c(patients$FLEX_LUMBAR_LAT_LAB_CM_DERECHA,
                                    controls$FLEX_LUMBAR_LAT_LAB_CM_DERECHA)

example <- data.frame(
  'manual' = FLEX_LUMBAR_LAT_MANUAL_CM_DERECHA,
  'lab' = FLEX_LUMBAR_LAT_LAB_CM_DERECHA
)

example <- example %>%
  mutate(diff = manual - lab)

model_FLEX_LUMBAR_LAT_LAB_CM_DERECHA <- lm(lab ~ manual, data = example)
summary(model_FLEX_LUMBAR_LAT_LAB_CM_DERECHA)

example %>%
  ggplot(aes(x = manual, y = lab)) +
  geom_point(aes(size = abs(diff))) +
  stat_smooth(method = 'lm') 

```



```{r}
DISTANCIA_INTERMALEOLAR_SI <- datos %>%
  filter(CONTROLADO == 'SI') %>%
  pull(DISTANCIA_INTERMALEOLAR) 

DISTANCIA_INTERMALEOLAR_NO <- datos %>%
  filter(CONTROLADO == 'NO') %>%
  pull(DISTANCIA_INTERMALEOLAR) 
  
shap_wilk_dist_intermaleolar_si <- shapiro.test(DISTANCIA_INTERMALEOLAR_SI)
shap_wilk_dist_intermaleolar_no <- shapiro.test(DISTANCIA_INTERMALEOLAR_NO)  

shap_wilk_dist_intermaleolar_si[['p.value']]
shap_wilk_dist_intermaleolar_no[['p.value']]

datos %>%
  group_by(CONTROLADO) %>%
  summarize(mean = mean(DISTANCIA_INTERMALEOLAR), sd = sd(DISTANCIA_INTERMALEOLAR), n = n())

```


```{r}
bootstrap <- function(vect){
    n <- length(vect)
    bootstrap_indices <- sample(x = 1:n, size = n, replace = TRUE)
    vect[bootstrap_indices]
}
```



```{r}
bootstrapped_sample_means <- replicate(n = 1E3, expr = mean(bootstrap(datos$PESO_kg)))

bootstrap_sd <- sd(bootstrapped_sample_means)
bootstrap_sd
```


# Models

## Logistic Regression (Naive approach)

Full model:

```{r}
full_naive_model_log_reg <- glm(CONTROLADO ~ .,
                                data = datos, family = "binomial")

summary(full_naive_model_log_reg)

```
It seems that `PESO_kg`, `ROTA_CERV_IZQ`, and `FLEX_FRONT_CERV_FLEXION` are the most relevant variables.

Confusion Matrix:

```{r}
full_naive_model_log_reg_pred <- predict(full_naive_model_log_reg, datos)
full_naive_model_log_reg_pred <- as_factor(if_else(full_naive_model_log_reg_pred < 0.5, 'SI', 'NO'))

Conf_Mat_full_naive <- confusionMatrix(full_naive_model_log_reg_pred, datos$CONTROLADO)

```


ROC:

```{r}
get_roc_curve(full_naive_model_log_reg)
```




### ROC curve and Sub-Models

Cutoffs: 0.2, 0.3, 0.5, 0.7, 0.90.

Function to get a roc curve (`get_roc_curve()`):


```{r eval = F}

get_roc_curve <- function(mod){
  
  test_pred_20 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_20 <- as_factor(if_else(test_pred_20 < 0.2, 'SI', 'NO'))
  
  
  test_pred_30 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_30 <- as_factor(if_else(test_pred_30 < 0.3, 'SI', 'NO'))
  
  #test_pred_40 <- predict(mod,
  #                        patients, type = 'response')
  #test_pred_40 <- as_factor(if_else(test_pred_40 < 0.4, 'SI', 'NO'))
  
  
  test_pred_50 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_50 <- as_factor(if_else(test_pred_50 < 0.5, 'SI', 'NO'))
  
  #test_pred_60 <- predict(mod,
  #                        patients, type = 'response')
  #test_pred_60 <- as_factor(if_else(test_pred_60 < 0.6, 'SI', 'NO'))
  
  test_pred_70 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_70 <- as_factor(if_else(test_pred_70 < 0.7, 'SI', 'NO'))
  
  #test_pred_80 <- predict(mod,
  #                        patients, type = 'response')
  #test_pred_80 <- as_factor(if_else(test_pred_80 < 0.8, 'SI', 'NO'))
  
  test_pred_90 <- predict(mod,
                          datos, type = 'response')
  
  test_pred_90 <- as_factor(if_else(test_pred_90 < 0.9, 'SI', 'NO'))
  
  #test_pred_95 <- predict(mod,
  #                        patients, type = 'response')
  
  #test_pred_95 <- as_factor(if_else(test_pred_95 < 0.95, 'SI', 'NO'))
  
  # Confusion Matrix:
  
  test_con_mat_20 = confusionMatrix(test_pred_20,
                                    datos$CONTROLADO)

  test_con_mat_30 = confusionMatrix(test_pred_30,
                                    datos$CONTROLADO)
  #test_con_mat_40 = confusionMatrix(test_pred_40,
  #                                  patients$CONTROLADO)
  test_con_mat_50 = confusionMatrix(test_pred_50,
                                    datos$CONTROLADO)
  #test_con_mat_60 = confusionMatrix(test_pred_60,
  #                                  patients$CONTROLADO)
  test_con_mat_70 = confusionMatrix(test_pred_70,
                                    datos$CONTROLADO)
  #test_con_mat_80 = confusionMatrix(test_pred_80,
  #                                  patients$CONTROLADO)
  test_con_mat_90 = confusionMatrix(test_pred_90,
                                    datos$CONTROLADO)
  #test_con_mat_95 = confusionMatrix(test_pred_95,
  #                                  patients$CONTROLADO)
  
  # Metrics:
  
  metrics = rbind(
  
  c(test_con_mat_20$overall["Accuracy"], 
    test_con_mat_20$byClass["Sensitivity"], 
    test_con_mat_20$byClass["Specificity"]),
  
  c(test_con_mat_30$overall["Accuracy"], 
    test_con_mat_30$byClass["Sensitivity"], 
    test_con_mat_30$byClass["Specificity"]),
  
  #c(test_con_mat_40$overall["Accuracy"], 
  #  test_con_mat_40$byClass["Sensitivity"], 
  #  test_con_mat_40$byClass["Specificity"]),
  
  c(test_con_mat_50$overall["Accuracy"], 
    test_con_mat_50$byClass["Sensitivity"], 
    test_con_mat_50$byClass["Specificity"]),
  
  #c(test_con_mat_60$overall["Accuracy"], 
  #  test_con_mat_60$byClass["Sensitivity"], 
  #  test_con_mat_60$byClass["Specificity"]),
  
  c(test_con_mat_70$overall["Accuracy"], 
    test_con_mat_70$byClass["Sensitivity"], 
    test_con_mat_70$byClass["Specificity"]),
  
  #c(test_con_mat_80$overall["Accuracy"], 
  #  test_con_mat_80$byClass["Sensitivity"], 
  #  test_con_mat_80$byClass["Specificity"]),
  
  c(test_con_mat_90$overall["Accuracy"], 
    test_con_mat_90$byClass["Sensitivity"], 
    test_con_mat_90$byClass["Specificity"])#,
  
  #c(test_con_mat_95$overall["Accuracy"], 
  #  test_con_mat_95$byClass["Sensitivity"], 
  #  test_con_mat_95$byClass["Specificity"]),
  )
  
  #rownames(metrics) = c('c = 0.20', 'c = 0.30', 'c = 0.40',
  #                      'c = 0.50', 'c = 0.60', 'c = 0.70',
  #                      'c = 0.80', 'c = 0.90', 'c = 0.95')
  
  rownames(metrics) = c('c = 0.20', 'c = 0.30', 'c = 0.50',
                        'c = 0.70', 'c = 0.90')
  
  # ROC curve:
  
  test_prob = predict(mod,
                      newdata = datos,
                      type = "response")
  
  test_roc = roc(datos$CONTROLADO ~ test_prob, plot = TRUE, print.auc = TRUE)
  
}

```


#### Cervical Rotation (Right-side)



```{r}

ROTA_CERV_DER_log_reg <- glm(CONTROLADO ~ ROTA_CERV_DER,
                             data = datos, family = "binomial")

get_roc_curve(ROTA_CERV_DER_log_reg)

```

Summary:

```{r}
summary(ROTA_CERV_DER_log_reg)
```

Graph:

```{r}
datos %>%
  ggplot(aes(x = ROTA_CERV_DER, y = CONTROLADO)) +
  geom_point(aes(colour = CONTROLADO))
```


Predictions:

```{r}
ROTA_CERV_DER_log_reg_pred <- predict(ROTA_CERV_DER_log_reg, datos)
ROTA_CERV_DER_log_reg_pred <- as_factor(if_else(ROTA_CERV_DER_log_reg_pred < 0.5, 'SI', 'NO'))

confusionMatrix(ROTA_CERV_DER_log_reg_pred, datos$CONTROLADO)
```



#### Cervical Rotation (Left-side)

Then,

```{r}

ROTA_CERV_IZQ_log_reg <- glm(CONTROLADO ~ ROTA_CERV_IZQ,
                             data = datos, family = "binomial")

get_roc_curve(ROTA_CERV_IZQ_log_reg)

```

#### Modified Schobert

Then,

```{r}

SCHOBERT_MOD_log_reg <- glm(CONTROLADO ~ SCHOBERT_MOD,
                            data = datos, family = "binomial")

get_roc_curve(SCHOBERT_MOD_log_reg)

```

Predictions:

```{r}
SCHOBERT_MOD_log_reg_pred <- predict(SCHOBERT_MOD_log_reg, datos)
SCHOBERT_MOD_log_reg_pred <- as_factor(if_else(SCHOBERT_MOD_log_reg_pred < 0.5, 'SI', 'NO'))

confusionMatrix(SCHOBERT_MOD_log_reg_pred, datos$CONTROLADO)
```

#### Intermaleolar Distance


```{r}

DISTANCIA_INTERMALEOLAR_log_reg <- glm(CONTROLADO ~ DISTANCIA_INTERMALEOLAR,
                                       data = datos, family = "binomial")

get_roc_curve(DISTANCIA_INTERMALEOLAR_log_reg)

```


```{r}
patients_A_full_log_reg <- glm(CONTROLADO ~ ROTA_CERV_DER +
                               ROTA_CERV_IZQ +
                               FLEX_FRONT_CERV_FLEXION +
                               FLEX_FRONT_CERV_EXTENSION +
                               FLEX_LAT_CERV_DERECHA +
                               FLEX_LAT_CERV_IZQUIERDA +
                               FLEX_LUMBAR_LAT_IZQUIERDA,
                             data = datos, family = "binomial")

get_roc_curve(patients_A_full_log_reg)
```




```{r}
patients_B_full_log_reg <- glm(CONTROLADO ~ ROTA_CERV_DER +
                               ROTA_CERV_IZQ +
                               FLEX_FRONT_CERV_FLEXION +
                               FLEX_FRONT_CERV_EXTENSION +
                               FLEX_LAT_CERV_DERECHA +
                               FLEX_LAT_CERV_IZQUIERDA +
                               FLEX_LUMBAR_LAT_IZQUIERDA +
                               DISTANCIA_INTERMALEOLAR,
                             data = datos, family = "binomial")

get_roc_curve(patients_B_full_log_reg)
```

```{r}
patients_B_full_log_reg_pred <- predict(patients_B_full_log_reg, datos,
                                        type = 'response')

patients_B_full_log_reg_pred <- as_factor(if_else(patients_B_full_log_reg_pred < 0.5,
                                                  'SI', 'NO'))

confusionMatrix(patients_B_full_log_reg_pred, datos$CONTROLADO)

```

## Logistic Bayesian Regression 


Bayesian regression [@burkner2017brms]:

### Prior distributions

* `PESO_kg`:

```{r}
shapiro.test(datos$PESO_kg)
```


The data comes from a normal distribution.


```{r}
datos %>%
  ggplot(aes(x = PESO_kg)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666") +
  #stat_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean(PESO_kg), colour = 'mean mass')) +
  geom_label(x = 100, y = 0.04, aes(label = paste('Mean: ', round(mean(PESO_kg),2)))) +
  geom_label(x = 100, y = 0.036, aes(label = paste('Std-Dev: ', round(sd(PESO_kg),2)))) +
  theme(legend.position = 'none')
```

* `ROTA_CERV_IZQ`:

```{r}
shapiro.test(datos$ROTA_CERV_IZQ)
```


The data comes from a normal distribution.


```{r}
datos %>%
  ggplot(aes(x = ROTA_CERV_IZQ)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666") +
  #stat_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean(ROTA_CERV_IZQ), colour = 'ROTA_CERV_IZQ')) +
  geom_label(x = 70, y = 0.058, aes(label = paste('Mean: ', round(mean(ROTA_CERV_IZQ),2)))) +
  geom_label(x = 70, y = 0.052, aes(label = paste('Std-Dev: ', round(sd(ROTA_CERV_IZQ),2)))) +
  theme(legend.position = 'none')
```

* `ROTA_CERV_DER`:


```{r}
shapiro.test(datos$ROTA_CERV_DER)
```

The data comes from a normal distribution.

```{r}
datos %>%
  ggplot(aes(x = ROTA_CERV_DER)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666") +
  #stat_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean(ROTA_CERV_DER), colour = 'ROTA_CERV_IZQ')) +
  geom_label(x = 70, y = 0.058, aes(label = paste('Mean: ', round(mean(ROTA_CERV_DER),2)))) +
  geom_label(x = 70, y = 0.052, aes(label = paste('Std-Dev: ', round(sd(ROTA_CERV_DER),2)))) +
  theme(legend.position = 'none')
```

* `SCHOBERT_MOD`

Then,

```{r}

SCHOBERT_MOD_STAN <- datos %>%
  select(SCHOBERT_MOD) %>%
  mutate(SCHOBERT_MOD_STAN = (SCHOBERT_MOD - mean(SCHOBERT_MOD))/sd(SCHOBERT_MOD)) %>%
  pull(SCHOBERT_MOD_STAN)


SCHOBERT_MOD_log_reg <- glm(CONTROLADO ~ SCHOBERT_MOD_STAN,
                            data = datos, family = "binomial")

get_roc_curve(SCHOBERT_MOD_log_reg)

```
```{r}
summary(SCHOBERT_MOD_log_reg)
```

Prior distributions:

```{r}
priors_SCHOBERT <- c(prior('normal(0, 1.1322)', class = 'Intercept'),
                     prior('normal(0, 0.9116)', class = 'b', coef = 'SCHOBERT_MOD_STAN'))
```

Then,

```{r}

data_SCHOBERT <- data.frame(
  'CONTROLADO' = if_else(datos$CONTROLADO == 'SI', 0, 1),
  'SCHOBERT_MOD_STAN' = SCHOBERT_MOD_STAN
)

SCHOBERT_MOD_BAYESIAN <- brm(CONTROLADO ~ 1 + SCHOBERT_MOD_STAN,
                             data = data_SCHOBERT, 
                             family = 'binomial',
                             cores = 4,
                             prior = priors_SCHOBERT)

summary(SCHOBERT_MOD_BAYESIAN)
```
Plots:

```{r}
plot(SCHOBERT_MOD_BAYESIAN)
```

* Prior:

```{r}
describe_prior(SCHOBERT_MOD_BAYESIAN)
```

* Posterior:

```{r}
describe_posterior(SCHOBERT_MOD_BAYESIAN)
```


Predictions:

```{r}
SCHOBERT_MOD_BAYESIAN_pred <- predict(SCHOBERT_MOD_BAYESIAN, data_SCHOBERT)

SCHOBERT_MOD_BAYESIAN_pred1 <- as_factor(if_else(SCHOBERT_MOD_BAYESIAN_pred[,1] < 0.5,
                                                  'SI', 'NO'))

confusionMatrix(SCHOBERT_MOD_BAYESIAN_pred1, datos$CONTROLADO)
```



* `FLEX_FRONT_CERV_FLEXION`:

```{r}
shapiro.test(datos$FLEX_FRONT_CERV_FLEXION)
```

The data comes from a normal distribution.

```{r}
datos %>%
  ggplot(aes(x = FLEX_FRONT_CERV_FLEXION)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666") +
  #stat_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean(FLEX_FRONT_CERV_FLEXION), colour = 'ROTA_CERV_IZQ')) +
  geom_label(x = 45, y = 0.11, aes(label = paste('Mean: ', round(mean(FLEX_FRONT_CERV_FLEXION),2)))) +
  geom_label(x = 45, y = 0.10, aes(label = paste('Std-Dev: ', round(sd(FLEX_FRONT_CERV_FLEXION),2)))) +
  theme(legend.position = 'none')
```

* `FLEX_FRONT_CERV_EXTENSION`:

```{r}
shapiro.test(datos$FLEX_FRONT_CERV_EXTENSION)
```

The data comes from a normal distribution.

```{r}
datos %>%
  ggplot(aes(x = FLEX_FRONT_CERV_EXTENSION)) +
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="#FF6666") +
  #stat_density(alpha = 0.5) +
  geom_vline(aes(xintercept = mean(FLEX_FRONT_CERV_EXTENSION, na.rm = T),
                 colour = 'FLEX_FRONT_CERV_EXTENSION')) +
  geom_label(x = 40, y = 0.10, aes(label = paste('Mean: ', round(mean(FLEX_FRONT_CERV_EXTENSION,
                                                                      na.rm = T),2)))) +
  geom_label(x = 40, y = 0.08, aes(label = paste('Std-Dev: ', round(sd(FLEX_FRONT_CERV_EXTENSION,
                                                                       na.rm = T),2)))) +
  theme(legend.position = 'none')
```

Hyper-parameters:

```{r}
summary(datos)
```


Then,

```{r}
full_bayesian_model_log_reg <- brm(CONTROLADO ~ .,
                                   data = datos,
                                   family = bernoulli(link = "logit"),
                                   chains = 4,
                                   cores = 4,
                                   seed = 17)

summary(full_bayesian_model_log_reg)
```



```{r fig.height=15, fig.width=15}
stanplot(full_bayesian_model_log_reg, 
         type = "trace")
```


```{r fig.height=10, fig.width=30}
stanplot(full_bayesian_model_log_reg, 
         type = "acf_bar")
```
```{r}
post <- posterior_samples(full_bayesian_model_log_reg)
```

```{r}
full_bayesian_model_log_reg_pred <- predict(full_bayesian_model_log_reg, datos, re_formula = NA) 

datos1 <- cbind(datos, full_bayesian_model_log_reg_pred)

datos1 <- datos1 %>%
  mutate(full_log_reg_bayesian_predictions = as_factor(if_else(Estimate < 0.5, 'SI', 'NO')))

confusionMatrix(datos1$CONTROLADO, datos1$full_log_reg_bayesian_predictions)

```

Then,

```{r}
#get_roc_curve(full_bayesian_model_log_reg)
```

## Subset Selection

```{r}
classfit_full <- regsubsets(CONTROLADO ~ ., datos, nvmax = 11)
summary_best_subset <- summary(classfit_full)
```

Performance:

```{r}
which.max(summary_best_subset$adjr2)
```
```{r}
par(mfrow = c(2, 2))
plot(summary_best_subset$rss, xlab = " Number of Variables ", ylab = " RSS ", type = "l")
plot(summary_best_subset$adjr2, xlab = " Number of Variables ", ylab = " Adjusted RSq ", type = "l")
points (5, summary_best_subset$adjr2[5], col = " red ", cex = 2, pch = 20)

plot(summary_best_subset$cp , xlab = " Number of Variables ", ylab = "Cp", type = "l")
which.min(summary_best_subset$cp)
points (2, summary_best_subset$cp[2] , col = " red ", cex = 2, pch = 20)
which.min(summary_best_subset$bic)

plot(summary_best_subset$bic, xlab = " Number of Variables ", ylab = " BIC ", type = "l")
```

Variables

```{r}
coef(classfit_full, 5)
```

```{r}
coef(classfit_full, 2)
```

For Logistic Regression:

Bestglm:

```{r}
data_bestglm <- within(datos, {
    y    <- CONTROLADO         # CONTROLADO into y
})

## Reorder variables
data_bestglm <-
    data_bestglm[, c("PESO_kg","ROTA_CERV_IZQ","ROTA_CERV_DER","FLEX_FRONT_CERV_FLEXION",
                     "FLEX_FRONT_CERV_EXTENSION","FLEX_LAT_CERV_DERECHA",
                     "FLEX_LAT_CERV_IZQUIERDA","SCHOBERT_MOD", "FLEX_LUMBAR_LAT_DERECHA",
                     "FLEX_LUMBAR_LAT_IZQUIERDA", "DISTANCIA_INTERMALEOLAR", "y")]
```

Then,

```{r}
results_bestglm <-
    bestglm(Xy = data_bestglm,
            family = binomial,
            IC = "AIC",                 # Information criteria for
            method = "exhaustive")
```


Another method:


```{r}
glmulti.logistic.out <-
    glmulti(CONTROLADO ~ ., data = datos,
            level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "bic",            # AIC as criteria
            confsetsize = 2,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # glm function
            family = binomial)       # binomial family for logistic regression

## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.logistic.out@formulas
```

gg
```{r}
summary(glmulti.logistic.out@objects[[1]])
```



## Regularization Methods

### Ridge

### The Lasso

## Support Vector Machines

## Tree-Based Methods



# References {-}

<div id="refs"></div>

# Appendix {-}